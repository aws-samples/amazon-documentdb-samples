{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip3.8 install pymongo boto3 langchain langchain_openai gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "import json\n",
    "import boto3\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
    "from langchain.document_loaders.text import TextLoader\n",
    "import gradio as gr\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pymongo import MongoClient \n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain_community.llms import Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a connection to your Amazon DocumentDB (MongoDB compatibility) cluster and creating the database\n",
    "client = pymongo.MongoClient(\n",
    "\"<Amazon DocumentDB database cluster connection string>\",\n",
    "port=27017,\n",
    "username=\"<username>\",\n",
    "password=\"<password>\",\n",
    "retryWrites=False,\n",
    "tls='true',\n",
    "tlsCAFile=\"/home/ec2-user/global-bundle.pem\") #Check the path as per your destination\n",
    "db = client.ragdemo\n",
    "collection = db.rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create connection to Amazon Bedrock - Omit if using OpenAI\n",
    "#Set the region as desired \n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime', region_name='select region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using OpenAI - Store API Key, Select and Configure models - Omit entire block if using Amazon Bedrock\n",
    "\"\"\"\n",
    "my_key= \"<your Open AI key>\"\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=my_key)\n",
    "llm = OpenAI(openai_api_key=my_key, temperature=0)\n",
    "\n",
    "# Load your .txt file by putting in the relevant path\n",
    "\n",
    "loader = TextLoader('transcript.txt') #you can use .txt file of your choice\n",
    "data = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "#Using MongoDB Langchain integration as DocumentDB is compatible with MongoDB insert API\n",
    "\n",
    "vectorStore = MongoDBAtlasVectorSearch.from_documents(documents=docs, embedding=embeddings, collection=collection) \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Omit entire block if using OpenAI \n",
    "\n",
    "model_id_embed = 'amazon.titan-embed-text-v1' \n",
    "accept = 'application/json' \n",
    "content_type = 'application/json'\n",
    "\n",
    "# Initialize BedrockEmbeddings client\n",
    "embeddings_client = BedrockEmbeddings(model_id=model_id_embed, client=bedrock_runtime)\n",
    "\n",
    "# Load your .txt file\n",
    "loader = TextLoader('transcript.txt')\n",
    "data = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "\n",
    "# Initialize a list to store embeddings from all chunks\n",
    "all_embeddings = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    # Extract 'page_content' for the current chunk\n",
    "    chunk_content = chunk.page_content\n",
    "\n",
    "    # Prepare the request body with the chunk's content\n",
    "    body = json.dumps({\n",
    "        \"inputText\": chunk_content,\n",
    "    })\n",
    "\n",
    "    # Check if the chunk content exceeds the maximum allowed length\n",
    "    if len(chunk_content) <= 50000:\n",
    "        # Invoke the model for the current chunk\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            body=body, \n",
    "            modelId=model_id, \n",
    "            accept=accept, \n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        # Extract the response for the current chunk\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        chunk_embeddings = response_body['embedding']\n",
    "\n",
    "        # Store the embeddings for the current chunk\n",
    "        all_embeddings.append(chunk_embeddings)\n",
    "    else:\n",
    "        # Handle the case where a single chunk exceeds the maximum allowed length\n",
    "        # You might need to split the chunk further or apply some other logic\n",
    "        print(f\"A chunk exceeded the maximum allowed length: {len(chunk_content)} characters\")\n",
    "\n",
    "#Using MongoDB Langchain integration as DocumentDB is compatible with MongoDB insert API\n",
    "\n",
    "vectorStore = MongoDBAtlasVectorSearch.from_documents(\n",
    "    documents=chunks, \n",
    "    embedding=embeddings_client,  \n",
    "    collection=collection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a HNSW vector search index. You can also create an ivfflat index.\n",
    "\n",
    "collection.create_index ([(\"embedding\",\"vector\")], \n",
    "    vectorOptions= {\n",
    "        \"type\": \"hnsw\", \n",
    "        \"similarity\": \"euclidean\",\n",
    "        \"dimensions\": 1536,\n",
    "        \"m\": 16,\n",
    "        \"efConstruction\": 64},\n",
    "    name=\"my_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and configure foundational model - Amazon Bedrock - Omit entire block if using OpenAI\n",
    "\n",
    "claude = Bedrock(\n",
    "   credentials_profile_name=\"default\", model_id=\"anthropic.claude-instant-v1\",\n",
    ")\n",
    "claude.model_kwargs = {'temperature': 0.0, 'max_tokens_to_sample': 1400,'top_k':10}\n",
    "\n",
    "llm=claude\n",
    "\n",
    "\n",
    "# Uncomment the following two lines to validate your connection if desired\n",
    "\"\"\"\n",
    "response = llm.invoke(\"Provide a list of the top agricultural products of Florida\")\n",
    "print(response) \n",
    "\"\"\"\n",
    "\n",
    "# Uncomment if desired to see which models are available to you from Amazon Bedrock\n",
    "\"\"\"\n",
    "bedrock_client = boto3.client(\n",
    "    service_name = \"bedrock\"\n",
    ")\n",
    "\n",
    "FM_list = bedrock_client.list_foundation_models()\n",
    "\n",
    "for model in FM_list['modelSummaries']:\n",
    "    print(model['modelId'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat function - Amazon Bedrock - Omit this block if using OpenAI \n",
    "\n",
    "chat_history = []\n",
    "\n",
    "def query_data(query, chat_history):\n",
    "\n",
    "    embedded_query = embeddings_client.embed_query(query)\n",
    "    docs = collection.aggregate([{'$search': {\"vectorSearch\" : {\"vector\" : embedded_query, \"path\": \"embedding\", \"similarity\": \"euclidean\", \"k\": 2}}}])\n",
    "    result = [doc['text'] for doc in docs]\n",
    "\n",
    "    # Create a PromptTemplate for the user's question\n",
    "    question_prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"query\", \"chat_history\"],\n",
    "        template=\"Given this text extracts:\\n-----\\n{context}\\n-----\\n and also consider the history of this chat {chat_history}\\nPlease answer the following question: {query}\",\n",
    "    )\n",
    "    # Create an LLMChain\n",
    "    llm_chain = LLMChain(prompt=question_prompt_template, llm=claude)\n",
    "\n",
    "    # Get the user's question and context documents\n",
    "    question = query\n",
    "    context_documents = result\n",
    "\n",
    "    # Prepare the input for the LLMChain\n",
    "    input_data = {\n",
    "        \"context\": \"\\n\".join(context_documents),\n",
    "        \"query\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "    }\n",
    "\n",
    "    # Run the LLMChain\n",
    "    output = llm_chain.run(input_data)\n",
    "    \n",
    "    return output\n",
    "\n",
    "#Uncomment output print function for debugging as required \n",
    "#print(query_data(\"What is the name of the company?\", chat_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat function - OpenAI - Omit if using using Amazon Bedrock \n",
    "\n",
    "\"\"\"\n",
    "chat_history = []\n",
    "\n",
    "def query_data(query, chat_history):\n",
    "    embedded_query = embeddings.embed_query(query)\n",
    "    docs = collection.aggregate([{'$search': {\"vectorSearch\" : {\"vector\" : embedded_query, \"path\": \"embedding\", \"similarity\": \"euclidean\", \"k\": 2}}}])\n",
    "    result = [doc['text'] for doc in docs]\n",
    "\n",
    "    # Create a PromptTemplate for the user's question\n",
    "    question_prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"query\", \"chat_history\"],\n",
    "        template=\"Given this text extracts:\\n-----\\n{context}\\n-----\\n and also consider the history of this chat {chat_history}\\nPlease answer the following question: {query}\",\n",
    "    )\n",
    "\n",
    "    # Create an LLMChain\n",
    "    llm_chain = LLMChain(prompt=question_prompt_template, llm=llm)\n",
    "\n",
    "    # Get the user's question and context documents\n",
    "    question = query\n",
    "    context_documents = result\n",
    "\n",
    "    # Prepare the input for the LLMChain\n",
    "    input_data = {\n",
    "        \"context\": \"\\n\".join(context_documents),\n",
    "        \"query\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "    }\n",
    "\n",
    "    # Run the LLMChain\n",
    "    output = llm_chain.run(input_data)\n",
    "    \n",
    "    return output\n",
    "\n",
    "#Uncomment output print function for debugging as required \n",
    "print(query_data(\"What is the name of the company?\", chat_history))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present the chatbot using Gradio.\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Amazon DocumentDB Powered Chatbot Demo\n",
    "    \"\"\")\n",
    "    gr.ChatInterface(query_data)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
