{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip3.8 install pymongo langchain gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "import pymongo\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
    "from langchain.document_loaders.text import TextLoader\n",
    "import gradio as gr\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a connection to your Amazon DocumentDB (MongoDB compatibility) cluster and creating the database\n",
    "client = pymongo.MongoClient(\n",
    "\"<Amazon DocumentDB database cluster connection string>\",\n",
    "port=27017,\n",
    "username=\"<username>\",\n",
    "password=\"<password>\",\n",
    "retryWrites=False,\n",
    "tls='true',\n",
    "tlsCAFile=\"/home/ec2-user/global-bundle.pem\") #Check the path as per your destination\n",
    "db = client.ragdemo\n",
    "collection = db.rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your OpenAI key\n",
    "my_key= \"<your Open AI key>\"\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=my_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your .txt file by putting in the relevant path\n",
    "loader = TextLoader('/home/ec2-user/sample_files/transcript.txt') #you can use .txt file of your choice\n",
    "data = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "#Using MongoDB Langchain integration as DocumentDB is compatible with MongoDB insert API\n",
    "vectorStore = MongoDBAtlasVectorSearch.from_documents(documents=docs, embedding=embeddings, collection=collection) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a HNSW vector search index. You can also create an ivfflat index.\n",
    "\n",
    "collection.create_index ([(\"embedding\",\"vector\")], \n",
    "    vectorOptions= {\n",
    "        \"type\": \"hnsw\", \n",
    "        \"similarity\": \"euclidean\",\n",
    "        \"dimensions\": 1536,\n",
    "        \"m\": 16,\n",
    "        \"efConstruction\": 64},\n",
    "    name=\"my_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your Open AI LLM model. In this case , we are using the default of Langchain.\n",
    "llm = OpenAI(openai_api_key=my_key, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat function\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "def query_data(query, chat_history):\n",
    "    embedded_query = embeddings.embed_query(query)\n",
    "    docs = collection.aggregate([{'$search': {\"vectorSearch\" : {\"vector\" : embedded_query, \"path\": \"embedding\", \"similarity\": \"euclidean\", \"k\": 2}}}])\n",
    "    result = [doc['text'] for doc in docs]\n",
    "\n",
    "    # Create a PromptTemplate for the user's question\n",
    "    question_prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"query\", \"chat_history\"],\n",
    "        template=\"Given this text extracts:\\n-----\\n{context}\\n-----\\n and also consider the history of this chat {chat_history}\\nPlease answer the following question: {query}\",\n",
    "    )\n",
    "\n",
    "    # Create an LLMChain\n",
    "    llm_chain = LLMChain(prompt=question_prompt_template, llm=llm)\n",
    "\n",
    "    # Get the user's question and context documents\n",
    "    question = query\n",
    "    context_documents = result\n",
    "\n",
    "    # Prepare the input for the LLMChain\n",
    "    input_data = {\n",
    "        \"context\": \"\\n\".join(context_documents),\n",
    "        \"query\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "    }\n",
    "\n",
    "    # Run the LLMChain\n",
    "    output = llm_chain.run(input_data)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present the chatbot using Gradio.\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Amazon DocumentDB Powered Chatbot Demo\n",
    "    \"\"\")\n",
    "    gr.ChatInterface(query_data)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
