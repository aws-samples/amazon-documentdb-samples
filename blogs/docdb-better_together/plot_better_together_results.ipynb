{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'logs'\n",
    "\n",
    "# Update with your logfile name\n",
    "scenario = 'scenario01_DOCDB_5371_9h55h92f.json'\n",
    "\n",
    "# It can analyze multple logfiles with the same extension by using a wild card\n",
    "# For example, to analye all files with extension _rv9ft04i at once.\n",
    "# scenario = 'scenario01_DOCDB_\\.*_rv9ft04i.json'\n",
    "\n",
    "data = {}\n",
    "\n",
    "def get_folders(folder):\n",
    "    return [f for f in os.listdir(folder) if os.path.isdir(os.path.join(folder, f))]\n",
    "\n",
    "def get_files(folder):\n",
    "    return [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "\n",
    "boxes = get_folders(folder)\n",
    "for box in boxes:\n",
    "    files = get_files(os.path.join(folder, box))\n",
    "    for file in files:\n",
    "        if re.match(scenario, file):\n",
    "            with open(os.path.join(folder, box, file), 'r') as f:\n",
    "                file_data = json.loads(f.read())\n",
    "                for timestamp in file_data.keys():\n",
    "                    datetime_str = datetime.datetime.fromtimestamp(int(timestamp)).strftime('%Y-%m-%d %H:%M:%S')                                                                                            \n",
    "                    if datetime_str not in data.keys():\n",
    "                        # Add a new entry but make sure no division by 0\n",
    "                        if file_data[timestamp]['count'] == 0:\n",
    "                            avg_time = 0\n",
    "                        else:\n",
    "                            avg_time = file_data[timestamp]['query_time'] / file_data[timestamp]['count']\n",
    "                        data[datetime_str] = {\n",
    "                            'count': file_data[timestamp]['count'],\n",
    "                            'query_time': file_data[timestamp]['query_time'],\n",
    "                            'min_time': file_data[timestamp]['min_time'],\n",
    "                            'max_time': file_data[timestamp]['max_time'],\n",
    "                            'avg_time': avg_time,\n",
    "                        }\n",
    "                    else:\n",
    "                        # An entry for this time stamp exists update the values\n",
    "                        data[datetime_str]['count'] += file_data[timestamp]['count']\n",
    "                        data[datetime_str]['query_time'] += file_data[timestamp]['query_time']                                                \n",
    "                        if file_data[timestamp]['min_time'] < data[datetime_str]['min_time']:\n",
    "                            data[datetime_str]['min_time'] = file_data[timestamp]['min_time']                            \n",
    "                        if file_data[timestamp]['max_time'] > data[datetime_str]['max_time']:\n",
    "                            data[datetime_str]['max_time'] = file_data[timestamp]['max_time']                        \n",
    "                        data[datetime_str]['avg_time'] = data[datetime_str]['query_time']/data[datetime_str]['count']\n",
    "\n",
    "\n",
    "# Order the data by timespamp. This can be usefull if multiple logsfies are used\n",
    "dataKeys = list(data.keys())\n",
    "dataKeys.sort()\n",
    "sortedData = {i: data[i] for i in dataKeys}\n",
    "data = sortedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a Pandas DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "# Convert the index to a DateTimeIndex\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Create a time series plot with multiple lines\n",
    "plt.plot(figsize=(8, 6))\n",
    "plt.plot(df.index, df['count'], label='count')\n",
    "# plt.plot(df.index, df['query_time'], label='query_time')\n",
    "\n",
    "# Add labels and title\n",
    "# plt.ylim(0, 20000)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('DocumentDB + EC Requests/s (count)')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.ylim(0, 0.02)\n",
    "#plt.plot(df.index, df['min_time'], label='min_time')\n",
    "#plt.plot(df.index, df['max_time'], label='max_time')\n",
    "plt.plot(df.index, df['avg_time'], label='avg_time')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('DocumentDB + EC Response Time (sec)')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "e123dbf39a817db7883dac87daf642ef9b3dab5ee93d94b2d80d8ddb7b50fde5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
