{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08b7a2fb-0746-45f4-b495-bf20e533fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 1\n",
    "from pymongo import MongoClient\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms import Bedrock\n",
    "from langchain.vectorstores import DocumentDBVectorSearch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import LLMChain\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6676a26-802e-4bb9-bae5-0bedc9953023",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 2\n",
    "#storing and retrieving DocumentDB credentials in AWS Secrets Manager see the following for more info in retrieving credentials\n",
    "#https://docs.aws.amazon.com/secretsmanager/latest/userguide/retrieving-secrets.html#retrieving-secrets-code\n",
    "\n",
    "secret_name = \"<secret name>\"\n",
    "region_name = \"<region>\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "session = boto3.session.Session()\n",
    "secrets_client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "try:\n",
    "    get_secret_value_response = secrets_client.get_secret_value(\n",
    "        SecretId=secret_name\n",
    "        )\n",
    "except ClientError as e:\n",
    "        # For a list of exceptions thrown, see\n",
    "        # https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "    raise e\n",
    "\n",
    "secret = json.loads(get_secret_value_response['SecretString'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cd1e559-2b43-4815-b0a5-d35c6600a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 3\n",
    "username = secret['username']\n",
    "password = secret['password']\n",
    "host = secret['host']\n",
    "port = secret['port']\n",
    "tls_file_location = '/path/to/tls/file.pem' #see https://docs.aws.amazon.com/documentdb/latest/developerguide/connect_programmatically.html#connect_programmatically-tls_enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f92ef5f3-447a-4aea-a450-7ef02beaf070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 4\n",
    "#see the following for information on connecting to DocumentDB: https://docs.aws.amazon.com/documentdb/latest/developerguide/connect_programmatically.html\n",
    "connection_string = f'mongodb://{username}:{password}@{host}:{port}/?tls=true&tlsCAFile={tls_file_location}&replicaSet=rs0&readPreference=secondaryPreferred&retryWrites=false'\n",
    "client = MongoClient(connection_string)\n",
    "database = client.database #pymongo utilizes dot notation, if a database is named \"database\", use client.database\n",
    "collection = database.collection #as above, utlize dot notation for collections with databases as follows database.collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fdd6520-2206-4f6c-943b-d182de387e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hnsw'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cell 5\n",
    "#below is creating a vector index named on field \"vectorContent\". By default, Langchain will insert chunks with the following fields: vectorContent, source, page, textContent\n",
    "# see the following for vector options in creating an index https://docs.aws.amazon.com/documentdb/latest/developerguide/vector-search.html#w5aac21c11c11, \n",
    "\n",
    "collection.create_index([(\"vectorContent\",\"vector\")], \n",
    "    vectorOptions= {\n",
    "        \"type\": \"hnsw\", \n",
    "        \"similarity\": \"<similarity>\",\n",
    "        \"dimensions\": 1536,\n",
    "        \"m\": 16,\n",
    "        \"efConstruction\": 64},\n",
    "    name=\"hnsw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3e11593-9c07-4511-adf3-b348e89f66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 6\n",
    "#initializing a boto3 Bedrock client to call LLM's with Bedrock https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime.html\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2073aeb5-d0d3-4e75-9a13-8d794306a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunking pdf files using Langchain Recursive text splitter and ingesting documents to DocumentDB, embeddings created with Amazon Titan\n",
    "\n",
    "files = '/path/to/files/' #just use the directory, do not use path to specific files\n",
    "\n",
    "loader = PyPDFDirectoryLoader(files)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    \n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "462894a9-8f34-4865-83c7-f81a3d5fb924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 7\n",
    "# initializing embeddings with Amazon Titan embeddings model\n",
    "#ingestion of chunked documents into DocumentDB vector search index that can be later used as a retriver for Retrieval QA\n",
    "embeddings = BedrockEmbeddings(model_id= \"amazon.titan-embed-text-v1\", client=bedrock_client)\n",
    "INDEX_NAME = \"hnsw\"\n",
    "vector_store = DocumentDBVectorSearch.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    collection=collection,\n",
    "    index_name=INDEX_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42cd386d-ff0a-4209-8592-f1cd7ac2d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 8\n",
    "#initilaizing an existing DocumentDB as a callable vector store, use this when you do not need to ingest documents like the code above\n",
    "name_space = \"database.collection\" #a namespace follows dot notation of a database and collection \n",
    "vector_store = DocumentDBVectorSearch.from_connection_string(\n",
    "    connection_string=connection_string,\n",
    "    namespace = name_space, \n",
    "    index_name = \"hnsw\",\n",
    "    embedding = embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dbdaf4a7-2e5d-4ffc-bc69-598a0bba643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 9\n",
    "#initializing Anthropic Claude for Amazon Bedrock as reasoning agent \n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2:1\", client=bedrock_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47d70175-39cc-4976-a35e-66256ff33f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 10\n",
    "#creating a prompt template https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3118aa34-e257-4bd0-8554-a980f8b22c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 11\n",
    "#initializing our DocumentDB vector store as a retriever\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26662d3-e2ab-4ca0-a21f-808c9e85e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 12\n",
    "#Using RetrievalQA chain to perform RAG. https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")\n",
    "\n",
    "output = qa.invoke({\"query\": \"<query>\"})\n",
    "\n",
    "print(\"query: \", output['query'])\n",
    "print(\"result: \" ,output[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379fa9d9-7b4d-4578-945f-3c4c4f5e4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 13\n",
    "#A different way to perform a rag that takes a query,chat history, context as arguments\n",
    "chat_history = []\n",
    "\n",
    "\n",
    "query = \"<query>\"\n",
    "docs = vector_store.similarity_search(query)\n",
    "result = [doc.page_content for doc in docs]\n",
    "\n",
    "# Create a PromptTemplate for the user's question\n",
    "question_prompt_template = PromptTemplate(\n",
    "input_variables=[\"context\", \"query\", \"chat_history\"],\n",
    "template=\"Given this text extracts:\\n-----\\n{context}\\n-----\\n and also consider the history of this chat {chat_history}\\nPlease answer the following question: {query}\",\n",
    "    )\n",
    "\n",
    "    # Create an LLMChain\n",
    "llm_chain = LLMChain(prompt=question_prompt_template, llm=llm)\n",
    "\n",
    "    # Get the user's question and context documents\n",
    "question = query\n",
    "context_documents = result\n",
    "\n",
    "    # Prepare the input for the LLMChain\n",
    "input_data = {\n",
    "        \"context\": \"\\n\".join(context_documents),\n",
    "        \"query\": question,\n",
    "        \"chat_history\": chat_history,\n",
    "    }\n",
    "\n",
    "    # Run the LLMChain\n",
    "output = llm_chain.invoke(input_data)\n",
    "print(\"context: \", output['context'])\n",
    "print(\"query: \", output['query'])\n",
    "print(\"output: \", output['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cefa51-5e39-4373-a78e-8b41d33981a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 14\n",
    "#performing vector search with Aggregation pipelines https://docs.aws.amazon.com/documentdb/latest/developerguide/vector-search.html#w5aac21c11c15\n",
    "embedded_query = embeddings.embed_query(\"<query>\")\n",
    "search = collection.aggregate([{'$search': {\"vectorSearch\" : {\"vector\" : embedded_query, \"path\": \"vectorContent\", \"similarity\": \"<similarity>\", \"k\": 5}}}])\n",
    "results = list(search)\n",
    "text = [result['textContent'] for result in results]\n",
    "print(text[0], \"\\n\")\n",
    "print(text[1], \"\\n\")\n",
    "print(text[2], \"\\n\")\n",
    "print(text[3], \"\\n\")\n",
    "print(text[4], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee9bab-0d2d-455d-adeb-8e7522a90255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
